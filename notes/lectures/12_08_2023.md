

## Memory Consistency

### Race Conditions

- A nondeterministic behavior that occurs in multithreaded programs

Race condition example:

thread 1 reads 0
thread 1 writes 1
thread 2 reads 1
thread 2 writes 2

After joining both threads, we read A
what value do we get? 1 or 2?

- Determined by which thread gets scheduled first.

Most memory models will enforce some store consistency:
- Writes will happen in some order and cannot be broken
  into multiple writes unless the data cannot fit in a single register.

### Data Races

- A data race occurs when two threads access the same memory location
  at the same time. (unsynchronized access to a single location and at least
  one of the threads writes).

- Data races do not neccessarily coincide with race conditions, but they
  often do.

- A data race is not a race condition.

Example that is both a race condition and a data race:

-**Sequential Consistency: all reads and writes occur in some order.**

Scenario: 4 global variables: A = B = X = Y = 0

thread 1
---------
A = 1
X = B

thread 2
---------
B = 1
Y = A

After we join both threads what are the values of X and Y?

- X = 1, Y = 1 (Both reads happen before both writes: A = 1, B = 1, X = B, Y = A)
- X = 1, Y = 0 (Order: B = 1, Y = A, A = 1, X = B)
- X = 0, Y = 1 (Order: A = 1, X = B, B = 1, Y = A)

**Under sequential consistency we cannot have a situation where both X and Y are 0.**

- If you draw it, there is a loop A = 1, X = B, B = 1, Y = A, A = 1, X = B, B = 1, Y = A, ...

**However no CPU guarantees sequential consistency.**

- There is no reason to wait for each operation in the thread to complete before
  moving on to the next operation, especially if the operations are not interrelated.

- So guaranteeing sequential consistency would be a waste of time (it would literally
  wait for each operation to complete before moving on to the next operation).

How do we solve the problem caused by a lack of sequential consistency?

- For efficency reasons, architectures allow CPUs to have briefly inconsistent
  memory states.

Intel CPUs use Total Store Ordering (TSO):

- Consistent ordering of writes, but writes may not immeadiately be visible
  to other CPUs.

Solution: 
- Memory barriers and memory fences

A memory fence is a special instruction that delays the CPU
until all previous memory writes have completed.

- Memory fences allow us to have sequential consistency when we need it,
  without paying for it in cases where we don't.

Atomic operations such as test-and-set use similar mechanisms to enforce sequential
consistency.

Using fences directly is not a good solution:
- Using too many fences can slow down the program.
- Not using enough fences can cause data races.

**We instead use multithreading libraries to but the fences in the correct places.**

Using the libraries does not guarantee data race prevention,
but it does make the behavior of the code more predictable.

## Deadlock

- A situation where two or more threads are waiting for each other
  to complete.

A deadlocked program is stuck and unable to continue.

Ex:
    some mutex M
    lock(M)
    lock(M) <- blocks forever

We are trying to acquire the lock which is already held by
another thread (in this case yourself).

A checked mutex may detect this behavior, but a fast mutex
would not have enough information to detect it.


**4 Conditions for Deadlock**

1) We need mutually exclusive access to the resource (USING MUTEX gives us this)
2) No preemption (only the thread that holds the resource can release it) (USING MUTEX gives us this)

3) hold and wait (while holding one resources, you can acquire another resource)
4) circular wait (A waits for B, B waits for C, C waits for A)

Suppose we have some struct

```
struct {
    mutex_t L;
    int value;
} account_t;

void transfer(account_t *from, account_t *to, int amount){
    
    lock(&from->L);
    
    if( from->value >= amount){
        lock(&to->L);
        to->value += amount;
        from->value -= amount;
        unlock(&to->L);
    }

    unlock(&from->L);


}


```

thread A
--------
transfer(&from, &to, 10)
lock(from->L)
lock(to->L)

thread B
--------
transfer(&to, &from, 10)
lock(to->L)
lock(from->L)

Each thread is blocked, waiting for the other.

Once we are in deadlock, it is too late

**We can try to avoid deadlock by not holding multiple resources at the same time**
- This is not practical nor efficient.

- Ensure an ordering when acquireming multiple resources.

Ex: 
- Imagine that each bank account has an associated ID.
Rule: Always get the lock for the lower ID first.
- **A consistent acquistion order prevents circular waiting**

Both threads will try to acquire A first, then B. (if the ID of A is lower)


Another options is to make the lock acquisition an atomic operation.
- This requires library support. (try-lock)

**Solving deadlocks does not fix memory consistency issues and vice versa.**

---

## Semaphores

- A mechanism for controlling access to a resource. (another form of synchornization)

These are the oldest method of synchronizing access to a resource.

- Mutex and condition variables can be made with semaphores.

A semaphore is an integer with two operations:
- post (increment): add 1 to the value
- wait (decrement): subtract 1 from the value (if value > 0)

If the semaphore is 0 and we try to decrease it, it will block until semaphore is > 0
(which will happen if another thread posts).

A mutex is like a semaphore that is initially 1.
- lock corresponds to wait
- unlock corresponds to post

**The difference is that with a mutex, only the thread which holds a lock
can post. In a semaphore, all threads can post.**

We can use a semaphore to block until some conditions are met.

---

Consider a bounded queue:
- we have two resources: 
- available data
- capacity

The [semaphore.h] header file defines the following functions:

int sem_init(sem_t *sem, int pshared, unsigned int value);
int sem_destroy(sem_t *sem);
int sem_wait(sem_t *sem);
int sem_post(sem_t *sem);

We could model a queue with 3 semaphores:

```
typedef struct {
    
    data_t array;
    sem_t space;
    sem_t data;
    sem_t lock;
    int head;
    int tail;

} queue_t;

void init(queue_t *Q){

    sem_init(&Q->space, 0, QUEUE_LENGTH);
    sem_init(&Q->data, 0, 0);
    sem_init(&Q->lock, 0, 1);
    Q->head = 0;
    Q->tail = 0;

}

void enqueue(queue_t *Q, data_t D){

    sem_wait(&Q->space);

    sem_wait(&Q->lock);

        Q->array[Q->tail] = D;
        Q->tail = (Q->tail + 1) % QUEUE_LENGTH;
        sem_post(&Q->lock);


    sem_post(&Q->lock);

    sem_post(&Q->data);




}

```

What are the differences between semaphores and mutexes (in POSIX)?

- Semaphores can be safely used in signal handlers
- We can open shared semaphores which are open for all processes. (sem_open)
- any thread can post to a semaphore, no concept for "holding" a semaphore.

Do semaphores solve deadlocks?

- Yes, but they do not solve memory consistency issues. (defeats the point
  of mutual exclusion)


---

